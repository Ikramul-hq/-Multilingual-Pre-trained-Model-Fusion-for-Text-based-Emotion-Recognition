{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-27T20:07:13.146835Z",
     "iopub.status.busy": "2025-01-27T20:07:13.146545Z",
     "iopub.status.idle": "2025-01-27T20:07:19.801885Z",
     "shell.execute_reply": "2025-01-27T20:07:19.800972Z",
     "shell.execute_reply.started": "2025-01-27T20:07:13.146813Z"
    },
    "id": "KI3yCJWMQ5WD",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 1. Imports & Setup (Same as before)\n",
    "# -------------------------------------------------------\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-01-27T20:07:19.803398Z",
     "iopub.status.busy": "2025-01-27T20:07:19.802916Z",
     "iopub.status.idle": "2025-01-27T20:07:19.890343Z",
     "shell.execute_reply": "2025-01-27T20:07:19.889429Z",
     "shell.execute_reply.started": "2025-01-27T20:07:19.803374Z"
    },
    "id": "PX8gy3z6RS6b",
    "outputId": "69de4dce-fd93-4fe8-b604-bcc8d2fe76a8",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7b2af5386f10>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-27T20:07:19.892091Z",
     "iopub.status.busy": "2025-01-27T20:07:19.891799Z",
     "iopub.status.idle": "2025-01-27T20:07:19.961824Z",
     "shell.execute_reply": "2025-01-27T20:07:19.961025Z",
     "shell.execute_reply.started": "2025-01-27T20:07:19.892061Z"
    },
    "id": "pT_l4ECRRbKN",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "TRAIN_PATH = \"/kaggle/input/ptbrdataset/ptbrtrain.csv\"\n",
    "VAL_PATH   = \"/kaggle/input/ptbrdataset/ptbrval.csv\"\n",
    "TEST_PATH  = \"/kaggle/input/ptbrdataset/ptbrtest.csv\"\n",
    "\n",
    "train_df = pd.read_csv(TRAIN_PATH)\n",
    "val_df   = pd.read_csv(VAL_PATH)\n",
    "test_df  = pd.read_csv(TEST_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-27T20:07:19.963277Z",
     "iopub.status.busy": "2025-01-27T20:07:19.963026Z",
     "iopub.status.idle": "2025-01-27T20:07:19.983698Z",
     "shell.execute_reply": "2025-01-27T20:07:19.982869Z",
     "shell.execute_reply.started": "2025-01-27T20:07:19.963258Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>anger</th>\n",
       "      <th>disgust</th>\n",
       "      <th>fear</th>\n",
       "      <th>joy</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ptbr_train_track_a_00001</td>\n",
       "      <td>Moça eu fiz uma análise com o Tinder uma vez e...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ptbr_train_track_a_00002</td>\n",
       "      <td>eles sempre mostram a míseria, em como tudo er...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ptbr_train_track_a_00003</td>\n",
       "      <td>eu nunca quis tanto algo como quero você. jama...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ptbr_train_track_a_00004</td>\n",
       "      <td>esperando aqui o stf vim me emparedar do eu ex...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ptbr_train_track_a_00005</td>\n",
       "      <td>e no final o PS vai pagar o pato, ja é o 4° tr...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         id  \\\n",
       "0  ptbr_train_track_a_00001   \n",
       "1  ptbr_train_track_a_00002   \n",
       "2  ptbr_train_track_a_00003   \n",
       "3  ptbr_train_track_a_00004   \n",
       "4  ptbr_train_track_a_00005   \n",
       "\n",
       "                                                text  anger  disgust  fear  \\\n",
       "0  Moça eu fiz uma análise com o Tinder uma vez e...      0        0     0   \n",
       "1  eles sempre mostram a míseria, em como tudo er...      1        1     0   \n",
       "2  eu nunca quis tanto algo como quero você. jama...      0        0     0   \n",
       "3  esperando aqui o stf vim me emparedar do eu ex...      1        0     0   \n",
       "4  e no final o PS vai pagar o pato, ja é o 4° tr...      1        0     0   \n",
       "\n",
       "   joy  sadness  surprise  \n",
       "0    0        0         0  \n",
       "1    0        0         0  \n",
       "2    1        1         0  \n",
       "3    0        0         0  \n",
       "4    0        0         0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-27T20:07:19.984897Z",
     "iopub.status.busy": "2025-01-27T20:07:19.984591Z",
     "iopub.status.idle": "2025-01-27T20:07:19.988979Z",
     "shell.execute_reply": "2025-01-27T20:07:19.988108Z",
     "shell.execute_reply.started": "2025-01-27T20:07:19.984868Z"
    },
    "id": "rDWDRDJ5Renf",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Instead of a single LABEL_VAR, define multiple label columns:\n",
    "LABEL_COLS = [\"anger\", \"disgust\", \"fear\", \"joy\", \"sadness\", \"surprise\"]\n",
    "TEXT_VAR   = \"text\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Load Tokenizer & Model for Embedding Extraction\n",
    "# -------------------------------------------------------\n",
    "model_name = \"Hate-speech-CNERG/dehatebert-mono-portugese\"\n",
    "text_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "text_model = AutoModel.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect the counts of \"1\" for each label\n",
    "counts_ones = []\n",
    "for label in LABEL_COLS:\n",
    "    # Count how many rows in train_df have this label = 1\n",
    "    num_ones = (train_df[label] == 1).sum()\n",
    "    counts_ones.append((label, num_ones))\n",
    "\n",
    "# Build a DataFrame for plotting\n",
    "counts_df = pd.DataFrame(counts_ones, columns=[\"label\", \"count_of_1\"])\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.barplot(\n",
    "    x=\"label\",\n",
    "    y=\"count_of_1\",\n",
    "    data=counts_df,\n",
    "    palette=\"Set2\"\n",
    ")\n",
    "plt.title(\"Counts of 1 for Each Label\")\n",
    "plt.xlabel(\"Label\")\n",
    "plt.ylabel(\"Count of 1\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-01-27T20:07:47.481543Z",
     "iopub.status.busy": "2025-01-27T20:07:47.481324Z",
     "iopub.status.idle": "2025-01-27T20:08:31.184113Z",
     "shell.execute_reply": "2025-01-27T20:08:31.183452Z",
     "shell.execute_reply.started": "2025-01-27T20:07:47.481526Z"
    },
    "id": "2bTxzSgPR7Ss",
    "outputId": "a5015b79-4eba-4ab8-d213-1c3e93c82e0d",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting text embeddings: 100%|██████████| 2226/2226 [00:20<00:00, 107.08it/s]\n",
      "Extracting text embeddings: 100%|██████████| 200/200 [00:01<00:00, 109.34it/s]\n",
      "Extracting text embeddings: 100%|██████████| 2226/2226 [00:20<00:00, 106.69it/s]\n"
     ]
    }
   ],
   "source": [
    "# 4. Extract Text Embeddings (Same logic, but shorten max_length if needed)\n",
    "# -------------------------------------------------------\n",
    "def extract_text_embeddings(df, save_path, model, tokenizer):\n",
    "    if os.path.exists(save_path):\n",
    "        print(f\"Embeddings already exist at {save_path}\")\n",
    "        return torch.load(save_path)\n",
    "\n",
    "    embeddings = {}\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for idx, row in tqdm(df.iterrows(), desc=\"Extracting text embeddings\", total=len(df)):\n",
    "            text_sample = row[TEXT_VAR]\n",
    "            text_sample = text_sample if isinstance(text_sample, str) else \"\"\n",
    "\n",
    "            inputs = tokenizer(\n",
    "                text_sample,\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "                max_length=128,\n",
    "                return_tensors=\"pt\"\n",
    "            ).to(device)\n",
    "\n",
    "            outputs = model(**inputs)\n",
    "            cls_embedding = outputs.last_hidden_state[:, 0, :]\n",
    "            embeddings[idx] = cls_embedding.cpu()\n",
    "\n",
    "    torch.save(embeddings, save_path)\n",
    "    return embeddings\n",
    "\n",
    "train_text_embeddings = extract_text_embeddings(train_df, \"train_text_embeddings.pt\", text_model, text_tokenizer)\n",
    "val_text_embeddings   = extract_text_embeddings(val_df,   \"val_text_embeddings.pt\",   text_model, text_tokenizer)\n",
    "test_text_embeddings  = extract_text_embeddings(test_df,  \"test_text_embeddings.pt\",  text_model, text_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-01-27T20:08:31.185998Z",
     "iopub.status.busy": "2025-01-27T20:08:31.185724Z",
     "iopub.status.idle": "2025-01-27T20:08:32.168427Z",
     "shell.execute_reply": "2025-01-27T20:08:32.167715Z",
     "shell.execute_reply.started": "2025-01-27T20:08:31.185977Z"
    },
    "id": "ykURmiwuSCBQ",
    "outputId": "aef3bcf9-a5c1-441a-ef65-8af4f9314d18",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: torch.Size([2226, 768]) y_train: torch.Size([2226, 6])\n",
      "X_val:   torch.Size([200, 768]) y_val:   torch.Size([200, 6])\n",
      "X_test:  torch.Size([2226, 768])\n"
     ]
    }
   ],
   "source": [
    "# 5. Prepare Embeddings (CHANGED for Multi‐Label)\n",
    "# -------------------------------------------------------\n",
    "def prepare_text_embeddings(text_embeddings, df, label_cols=None, has_labels=True):\n",
    "    \"\"\"\n",
    "    For each row in df, gather the text embedding and (optionally) the labels.\n",
    "    label_cols: list of columns for multi-label (e.g. [\"anger\", \"fear\", \"joy\", ...])\n",
    "    \"\"\"\n",
    "    combined_embeddings = []\n",
    "    labels = []\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        if idx not in text_embeddings:\n",
    "            continue\n",
    "        # Get the [CLS] embedding\n",
    "        text_embedding = text_embeddings[idx].squeeze()  # shape: (768,)\n",
    "        combined_embeddings.append(text_embedding)\n",
    "\n",
    "        if has_labels and label_cols is not None:\n",
    "            # Collect all label columns as a float vector\n",
    "            label_vector = row[label_cols].values.astype(float)  # shape: (5,)\n",
    "            labels.append(label_vector)\n",
    "\n",
    "    # Convert to Tensors\n",
    "    if has_labels and label_cols is not None:\n",
    "        X = torch.stack(combined_embeddings)\n",
    "        Y = torch.tensor(labels, dtype=torch.float)  # multi-label => float\n",
    "        return X, Y\n",
    "    else:\n",
    "        return torch.stack(combined_embeddings)\n",
    "\n",
    "X_train, y_train = prepare_text_embeddings(train_text_embeddings, train_df, LABEL_COLS, has_labels=True)\n",
    "X_val,   y_val   = prepare_text_embeddings(val_text_embeddings,   val_df,   LABEL_COLS, has_labels=True)\n",
    "X_test           = prepare_text_embeddings(test_text_embeddings,  test_df,  LABEL_COLS, has_labels=False)\n",
    "\n",
    "print(\"X_train:\", X_train.shape, \"y_train:\", y_train.shape)\n",
    "print(\"X_val:  \", X_val.shape,   \"y_val:  \", y_val.shape)\n",
    "print(\"X_test: \", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-01-27T20:08:32.169552Z",
     "iopub.status.busy": "2025-01-27T20:08:32.169271Z",
     "iopub.status.idle": "2025-01-27T20:08:32.631981Z",
     "shell.execute_reply": "2025-01-27T20:08:32.631257Z",
     "shell.execute_reply.started": "2025-01-27T20:08:32.169523Z"
    },
    "id": "7oz_zljhMpnm",
    "outputId": "8831f5e6-7127-4836-80ee-2a3b1f0ae663",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset size: 2226\n",
      "Balanced dataset size: 19649\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def balance_multilabel_data(X, Y):\n",
    "    \"\"\"\n",
    "    Oversample minority label-combinations in a multi-label setting.\n",
    "    1. Convert each row's label vector (like [1,0,1,0,0]) into a tuple (1.0,0.0,1.0,0.0,0.0).\n",
    "    2. Count how many rows share that exact tuple (i.e., label combination).\n",
    "    3. Duplicate those rows until they match the frequency of the most common combination.\n",
    "\n",
    "    NOTE: This lumps each distinct 5-label pattern as one \"class.\"\n",
    "    If every row has a unique pattern, this won't help much.\n",
    "    \"\"\"\n",
    "    from collections import Counter\n",
    "\n",
    "    # Convert each label row to a tuple\n",
    "    label_tuples = [tuple(row.tolist()) for row in Y]\n",
    "    class_counts = Counter(label_tuples)\n",
    "    max_count = max(class_counts.values())\n",
    "\n",
    "    # We'll store duplicates in lists, then convert to tensors\n",
    "    balanced_embeddings = []\n",
    "    balanced_labels = []\n",
    "\n",
    "    for i, label_tuple in enumerate(label_tuples):\n",
    "        balanced_embeddings.append(X[i])\n",
    "        balanced_labels.append(label_tuple)\n",
    "\n",
    "        current_count = class_counts[label_tuple]\n",
    "        # e.g., duplicates_needed = how many times to replicate?\n",
    "        duplicates_needed = int((max_count - current_count) / current_count)\n",
    "\n",
    "        for _ in range(duplicates_needed):\n",
    "            balanced_embeddings.append(X[i])\n",
    "            balanced_labels.append(label_tuple)\n",
    "\n",
    "    balanced_embeddings = torch.stack(balanced_embeddings)\n",
    "    balanced_labels = torch.tensor(balanced_labels, dtype=torch.float)\n",
    "    print(f\"Original dataset size: {X.shape[0]}\")\n",
    "    print(f\"Balanced dataset size: {balanced_embeddings.shape[0]}\")\n",
    "    return balanced_embeddings, balanced_labels\n",
    "\n",
    "# Call our new multi-label balancing function\n",
    "X_train, y_train = balance_multilabel_data(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-27T20:08:32.633156Z",
     "iopub.status.busy": "2025-01-27T20:08:32.632832Z",
     "iopub.status.idle": "2025-01-27T20:08:32.638833Z",
     "shell.execute_reply": "2025-01-27T20:08:32.638163Z",
     "shell.execute_reply.started": "2025-01-27T20:08:32.633124Z"
    },
    "id": "j2JzYjEHSMu7",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 6. Define Multi‐Label MLP Model (CHANGED)\n",
    "# -------------------------------------------------------\n",
    "class MLPModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, dropout_p=0.5):\n",
    "        super(MLPModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim[0])\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(p=dropout_p)\n",
    "        self.fc2 = nn.Linear(hidden_dim[0], hidden_dim[1])\n",
    "        self.dropout2 = nn.Dropout(p=dropout_p)\n",
    "        self.fc3 = nn.Linear(hidden_dim[1], output_dim)\n",
    "        # For multi‐label, we *do not* apply softmax, we will use BCEWithLogitsLoss\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc3(x)            # shape: (batch_size, 5)\n",
    "        # No softmax here for multi‐label. Return raw logits for BCEWithLogitsLoss\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-27T20:08:32.639978Z",
     "iopub.status.busy": "2025-01-27T20:08:32.639637Z",
     "iopub.status.idle": "2025-01-27T20:08:32.667039Z",
     "shell.execute_reply": "2025-01-27T20:08:32.666257Z",
     "shell.execute_reply.started": "2025-01-27T20:08:32.639948Z"
    },
    "id": "TSkqLDOVSVyh",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Model & Hyperparams\n",
    "input_dim    = X_train.shape[1]        # e.g. 768\n",
    "hidden_dim   = [786, 512]             # can be tuned\n",
    "output_dim   = len(LABEL_COLS)        # 5 for multi‐label\n",
    "dropout_p    = 0.3\n",
    "num_epochs   = 50\n",
    "batch_size   = 16\n",
    "learning_rate = 1e-4\n",
    "\n",
    "model = MLPModel(input_dim, hidden_dim, output_dim, dropout_p).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-27T20:08:32.668254Z",
     "iopub.status.busy": "2025-01-27T20:08:32.667955Z",
     "iopub.status.idle": "2025-01-27T20:08:32.672741Z",
     "shell.execute_reply": "2025-01-27T20:08:32.671835Z",
     "shell.execute_reply.started": "2025-01-27T20:08:32.668227Z"
    },
    "id": "qeMpwVS2SYam",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 7. CHANGED: Use BCEWithLogitsLoss (for multi‐label)\n",
    "# -------------------------------------------------------\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-27T20:08:32.673881Z",
     "iopub.status.busy": "2025-01-27T20:08:32.673607Z",
     "iopub.status.idle": "2025-01-27T20:08:32.686875Z",
     "shell.execute_reply": "2025-01-27T20:08:32.686067Z",
     "shell.execute_reply.started": "2025-01-27T20:08:32.673858Z"
    },
    "id": "RIiqLiirSbo5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 8. Dataloaders\n",
    "# -------------------------------------------------------\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "val_dataset   = TensorDataset(X_val,   y_val)\n",
    "test_dataset  = TensorDataset(X_test)  # no labels for test\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=batch_size)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-27T20:08:32.687965Z",
     "iopub.status.busy": "2025-01-27T20:08:32.687685Z",
     "iopub.status.idle": "2025-01-27T20:08:32.698764Z",
     "shell.execute_reply": "2025-01-27T20:08:32.697999Z",
     "shell.execute_reply.started": "2025-01-27T20:08:32.687938Z"
    },
    "id": "7biPQklUSeXM",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 9. Metrics for Multi‐Label (CHANGED)\n",
    "# -------------------------------------------------------\n",
    "def calculate_metrics(preds, labels):\n",
    "    \"\"\"\n",
    "    preds, labels are lists (or arrays) of shape (N, 5).\n",
    "    We'll do an example with macro avg for PRF.\n",
    "    \"\"\"\n",
    "    preds = np.array(preds)\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    # Example: For each label, threshold at 0.5\n",
    "    # (We already do that during loop, but let's be explicit.)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        labels, preds, average='macro', zero_division=0\n",
    "    )\n",
    "    # Multi‐label “accuracy” can be ambiguous.\n",
    "    # Some do “exact match ratio”, etc. We'll do a simple overall average:\n",
    "    accuracy = (preds == labels).mean()\n",
    "\n",
    "    return accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2025-01-27T20:08:32.699920Z",
     "iopub.status.busy": "2025-01-27T20:08:32.699644Z",
     "iopub.status.idle": "2025-01-27T20:10:51.251380Z",
     "shell.execute_reply": "2025-01-27T20:10:51.250652Z",
     "shell.execute_reply.started": "2025-01-27T20:08:32.699895Z"
    },
    "id": "AHid0LPJShaz",
    "outputId": "3e765570-2ac8-45c2-8534-e782233435b9",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 Train Loss: 0.3606, Train Acc: 0.8392, F1: 0.7411 | Val Loss: 0.5504, Val Acc: 0.7525, F1: 0.2431\n",
      "Best model saved with F1: 0.2431 at epoch 1\n",
      "Epoch 2/50 Train Loss: 0.1932, Train Acc: 0.9249, F1: 0.8939 | Val Loss: 0.5242, Val Acc: 0.7667, F1: 0.2684\n",
      "Best model saved with F1: 0.2684 at epoch 2\n",
      "Epoch 3/50 Train Loss: 0.1345, Train Acc: 0.9505, F1: 0.9312 | Val Loss: 0.5339, Val Acc: 0.7775, F1: 0.2643\n",
      "Epoch 4/50 Train Loss: 0.1006, Train Acc: 0.9649, F1: 0.9519 | Val Loss: 0.5361, Val Acc: 0.7817, F1: 0.2458\n",
      "Epoch 5/50 Train Loss: 0.0782, Train Acc: 0.9733, F1: 0.9638 | Val Loss: 0.5491, Val Acc: 0.8050, F1: 0.2836\n",
      "Best model saved with F1: 0.2836 at epoch 5\n",
      "Epoch 6/50 Train Loss: 0.0630, Train Acc: 0.9793, F1: 0.9718 | Val Loss: 0.5761, Val Acc: 0.7958, F1: 0.2689\n",
      "Epoch 7/50 Train Loss: 0.0532, Train Acc: 0.9824, F1: 0.9761 | Val Loss: 0.5945, Val Acc: 0.8000, F1: 0.2544\n",
      "Epoch 8/50 Train Loss: 0.0459, Train Acc: 0.9847, F1: 0.9792 | Val Loss: 0.6056, Val Acc: 0.8008, F1: 0.2611\n",
      "Epoch 9/50 Train Loss: 0.0395, Train Acc: 0.9867, F1: 0.9818 | Val Loss: 0.6233, Val Acc: 0.8067, F1: 0.2637\n",
      "Epoch 10/50 Train Loss: 0.0354, Train Acc: 0.9881, F1: 0.9837 | Val Loss: 0.6555, Val Acc: 0.8025, F1: 0.2584\n",
      "Epoch 11/50 Train Loss: 0.0298, Train Acc: 0.9900, F1: 0.9860 | Val Loss: 0.6597, Val Acc: 0.8217, F1: 0.2739\n",
      "Epoch 12/50 Train Loss: 0.0283, Train Acc: 0.9904, F1: 0.9867 | Val Loss: 0.7395, Val Acc: 0.8117, F1: 0.2303\n",
      "Epoch 13/50 Train Loss: 0.0258, Train Acc: 0.9913, F1: 0.9882 | Val Loss: 0.7021, Val Acc: 0.8092, F1: 0.2498\n",
      "Epoch 14/50 Train Loss: 0.0227, Train Acc: 0.9924, F1: 0.9895 | Val Loss: 0.7086, Val Acc: 0.8158, F1: 0.2655\n",
      "Epoch 15/50 Train Loss: 0.0202, Train Acc: 0.9932, F1: 0.9907 | Val Loss: 0.7307, Val Acc: 0.8175, F1: 0.2637\n",
      "Epoch 16/50 Train Loss: 0.0197, Train Acc: 0.9934, F1: 0.9909 | Val Loss: 0.7277, Val Acc: 0.8142, F1: 0.2697\n",
      "Epoch 17/50 Train Loss: 0.0175, Train Acc: 0.9939, F1: 0.9916 | Val Loss: 0.8150, Val Acc: 0.8050, F1: 0.2425\n",
      "Epoch 18/50 Train Loss: 0.0158, Train Acc: 0.9946, F1: 0.9926 | Val Loss: 0.7600, Val Acc: 0.8175, F1: 0.2461\n",
      "Epoch 19/50 Train Loss: 0.0154, Train Acc: 0.9949, F1: 0.9928 | Val Loss: 0.8648, Val Acc: 0.8200, F1: 0.2593\n",
      "Epoch 20/50 Train Loss: 0.0147, Train Acc: 0.9949, F1: 0.9930 | Val Loss: 0.8537, Val Acc: 0.8208, F1: 0.2949\n",
      "Best model saved with F1: 0.2949 at epoch 20\n",
      "Epoch 21/50 Train Loss: 0.0132, Train Acc: 0.9954, F1: 0.9937 | Val Loss: 0.8527, Val Acc: 0.8283, F1: 0.2905\n",
      "Epoch 22/50 Train Loss: 0.0123, Train Acc: 0.9958, F1: 0.9942 | Val Loss: 0.9007, Val Acc: 0.8058, F1: 0.2313\n",
      "Epoch 23/50 Train Loss: 0.0117, Train Acc: 0.9959, F1: 0.9945 | Val Loss: 0.9448, Val Acc: 0.8142, F1: 0.2446\n",
      "Epoch 24/50 Train Loss: 0.0109, Train Acc: 0.9963, F1: 0.9949 | Val Loss: 0.9021, Val Acc: 0.8200, F1: 0.2918\n",
      "Epoch 25/50 Train Loss: 0.0108, Train Acc: 0.9963, F1: 0.9949 | Val Loss: 0.9461, Val Acc: 0.8117, F1: 0.2606\n",
      "Epoch 26/50 Train Loss: 0.0099, Train Acc: 0.9964, F1: 0.9950 | Val Loss: 0.9496, Val Acc: 0.8225, F1: 0.2585\n",
      "Epoch 27/50 Train Loss: 0.0092, Train Acc: 0.9968, F1: 0.9957 | Val Loss: 0.9844, Val Acc: 0.8058, F1: 0.2549\n",
      "Epoch 28/50 Train Loss: 0.0097, Train Acc: 0.9967, F1: 0.9954 | Val Loss: 0.9902, Val Acc: 0.8133, F1: 0.2671\n",
      "Epoch 29/50 Train Loss: 0.0092, Train Acc: 0.9968, F1: 0.9956 | Val Loss: 1.0026, Val Acc: 0.8142, F1: 0.2655\n",
      "Epoch 30/50 Train Loss: 0.0094, Train Acc: 0.9968, F1: 0.9955 | Val Loss: 1.0049, Val Acc: 0.8300, F1: 0.2808\n",
      "Epoch 31/50 Train Loss: 0.0082, Train Acc: 0.9970, F1: 0.9959 | Val Loss: 1.0239, Val Acc: 0.8308, F1: 0.2820\n",
      "Epoch 32/50 Train Loss: 0.0081, Train Acc: 0.9973, F1: 0.9963 | Val Loss: 1.0571, Val Acc: 0.8200, F1: 0.2844\n",
      "Epoch 33/50 Train Loss: 0.0074, Train Acc: 0.9975, F1: 0.9965 | Val Loss: 1.0789, Val Acc: 0.8200, F1: 0.2467\n",
      "Epoch 34/50 Train Loss: 0.0074, Train Acc: 0.9974, F1: 0.9965 | Val Loss: 1.0280, Val Acc: 0.8150, F1: 0.2661\n",
      "Epoch 35/50 Train Loss: 0.0075, Train Acc: 0.9974, F1: 0.9964 | Val Loss: 1.1147, Val Acc: 0.8242, F1: 0.2614\n",
      "Epoch 36/50 Train Loss: 0.0063, Train Acc: 0.9979, F1: 0.9970 | Val Loss: 1.1503, Val Acc: 0.8167, F1: 0.2640\n",
      "Epoch 37/50 Train Loss: 0.0065, Train Acc: 0.9977, F1: 0.9967 | Val Loss: 1.1031, Val Acc: 0.8267, F1: 0.2810\n",
      "Epoch 38/50 Train Loss: 0.0074, Train Acc: 0.9974, F1: 0.9964 | Val Loss: 1.1477, Val Acc: 0.8208, F1: 0.2126\n",
      "Epoch 39/50 Train Loss: 0.0067, Train Acc: 0.9977, F1: 0.9969 | Val Loss: 1.1411, Val Acc: 0.8200, F1: 0.2260\n",
      "Epoch 40/50 Train Loss: 0.0062, Train Acc: 0.9978, F1: 0.9970 | Val Loss: 1.1177, Val Acc: 0.8158, F1: 0.2323\n",
      "Epoch 41/50 Train Loss: 0.0057, Train Acc: 0.9981, F1: 0.9974 | Val Loss: 1.1446, Val Acc: 0.8242, F1: 0.2422\n",
      "Epoch 42/50 Train Loss: 0.0054, Train Acc: 0.9982, F1: 0.9975 | Val Loss: 1.2549, Val Acc: 0.8333, F1: 0.2794\n",
      "Epoch 43/50 Train Loss: 0.0061, Train Acc: 0.9979, F1: 0.9972 | Val Loss: 1.2493, Val Acc: 0.8158, F1: 0.2776\n",
      "Epoch 44/50 Train Loss: 0.0057, Train Acc: 0.9981, F1: 0.9973 | Val Loss: 1.2379, Val Acc: 0.8117, F1: 0.2788\n",
      "Epoch 45/50 Train Loss: 0.0053, Train Acc: 0.9981, F1: 0.9974 | Val Loss: 1.1744, Val Acc: 0.8300, F1: 0.2744\n",
      "Epoch 46/50 Train Loss: 0.0054, Train Acc: 0.9981, F1: 0.9973 | Val Loss: 1.2939, Val Acc: 0.8183, F1: 0.2000\n",
      "Epoch 47/50 Train Loss: 0.0055, Train Acc: 0.9981, F1: 0.9973 | Val Loss: 1.2974, Val Acc: 0.8192, F1: 0.2497\n",
      "Epoch 48/50 Train Loss: 0.0066, Train Acc: 0.9978, F1: 0.9971 | Val Loss: 1.2412, Val Acc: 0.8317, F1: 0.2858\n",
      "Epoch 49/50 Train Loss: 0.0050, Train Acc: 0.9983, F1: 0.9976 | Val Loss: 1.3445, Val Acc: 0.8200, F1: 0.3058\n",
      "Best model saved with F1: 0.3058 at epoch 49\n",
      "Epoch 50/50 Train Loss: 0.0056, Train Acc: 0.9980, F1: 0.9972 | Val Loss: 1.2570, Val Acc: 0.8208, F1: 0.2487\n",
      "Best model saved at: ./models/best_model_epoch_49_f1_0.3058.pth\n"
     ]
    }
   ],
   "source": [
    "# 10. Train and Validate (CHANGES in Predictions)\n",
    "# -------------------------------------------------------\n",
    "def train_and_save_best_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, save_dir):\n",
    "    best_f1 = -float('inf')\n",
    "    best_model_path = None\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        all_train_preds, all_train_labels = [], []\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)  # shape: (batch_size, 5)\n",
    "            # For BCEWithLogitsLoss, labels should be float\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            # Convert logits -> probabilities -> binary predictions\n",
    "            preds = (torch.sigmoid(outputs) > 0.5).float()\n",
    "            all_train_preds.extend(preds.cpu().tolist())\n",
    "            all_train_labels.extend(labels.cpu().tolist())\n",
    "\n",
    "        # Training metrics\n",
    "        train_accuracy, train_precision, train_recall, train_f1 = calculate_metrics(all_train_preds, all_train_labels)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        all_val_preds, all_val_labels = [], []\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)  # shape: (batch_size, 5)\n",
    "\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                preds = (torch.sigmoid(outputs) > 0.5).float()\n",
    "                all_val_preds.extend(preds.cpu().tolist())\n",
    "                all_val_labels.extend(labels.cpu().tolist())\n",
    "\n",
    "        val_accuracy, val_precision, val_recall, val_f1 = calculate_metrics(all_val_preds, all_val_labels)\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch+1}/{num_epochs} \"\n",
    "            f\"Train Loss: {train_loss/len(train_loader):.4f}, \"\n",
    "            f\"Train Acc: {train_accuracy:.4f}, F1: {train_f1:.4f} | \"\n",
    "            f\"Val Loss: {val_loss/len(val_loader):.4f}, \"\n",
    "            f\"Val Acc: {val_accuracy:.4f}, F1: {val_f1:.4f}\"\n",
    "        )\n",
    "\n",
    "        # Save best model by val_f1\n",
    "        if val_f1 > best_f1:\n",
    "            best_f1 = val_f1\n",
    "            best_model_path = os.path.join(save_dir, f\"best_model_epoch_{epoch+1}_f1_{val_f1:.4f}.pth\")\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "            print(f\"Best model saved with F1: {val_f1:.4f} at epoch {epoch+1}\")\n",
    "\n",
    "    return best_model_path\n",
    "\n",
    "save_dir = \"./models\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "best_model_path = train_and_save_best_model(\n",
    "    model, train_loader, val_loader, criterion, optimizer, num_epochs, save_dir\n",
    ")\n",
    "\n",
    "print(f\"Best model saved at: {best_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "execution": {
     "iopub.execute_input": "2025-01-27T20:10:51.252427Z",
     "iopub.status.busy": "2025-01-27T20:10:51.252199Z",
     "iopub.status.idle": "2025-01-27T20:10:51.356951Z",
     "shell.execute_reply": "2025-01-27T20:10:51.356090Z",
     "shell.execute_reply.started": "2025-01-27T20:10:51.252408Z"
    },
    "id": "kvMYTGpMSmf9",
    "outputId": "8785d5d4-4b92-4e8d-f869-5fd273b2fff4",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission file saved to submission.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>anger_pred</th>\n",
       "      <th>disgust_pred</th>\n",
       "      <th>fear_pred</th>\n",
       "      <th>joy_pred</th>\n",
       "      <th>sadness_pred</th>\n",
       "      <th>surprise_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ptbr_test_track_a_00001</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ptbr_test_track_a_00002</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ptbr_test_track_a_00003</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ptbr_test_track_a_00004</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ptbr_test_track_a_00005</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        id  anger_pred  disgust_pred  fear_pred  joy_pred  \\\n",
       "0  ptbr_test_track_a_00001           0             0          0         0   \n",
       "1  ptbr_test_track_a_00002           0             0          0         1   \n",
       "2  ptbr_test_track_a_00003           0             0          0         1   \n",
       "3  ptbr_test_track_a_00004           0             0          0         0   \n",
       "4  ptbr_test_track_a_00005           0             0          0         1   \n",
       "\n",
       "   sadness_pred  surprise_pred  \n",
       "0             1              0  \n",
       "1             0              0  \n",
       "2             1              0  \n",
       "3             0              0  \n",
       "4             0              0  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 11. Test Predictions (CHANGES for Multi‐Label)\n",
    "# -------------------------------------------------------\n",
    "def predict_and_generate_submission(test_loader, best_model_path, submission_file_path):\n",
    "    # Reload a fresh model\n",
    "    inference_model = MLPModel(input_dim, hidden_dim, output_dim, dropout_p).to(device)\n",
    "    inference_model.load_state_dict(torch.load(best_model_path))\n",
    "    inference_model.eval()\n",
    "\n",
    "    # We'll store multi‐label predictions as 0/1 for each label\n",
    "    test_predictions = []\n",
    "    with torch.no_grad():\n",
    "        for (inputs,) in test_loader:  # Each batch is a tuple containing only X\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = inference_model(inputs)\n",
    "            preds = (torch.sigmoid(outputs) > 0.5).int()  # shape: (batch_size, 5)\n",
    "            test_predictions.append(preds.cpu())\n",
    "\n",
    "    test_predictions = torch.cat(test_predictions, dim=0).numpy()  # shape: (num_samples, 5)\n",
    "\n",
    "    # Build submission DataFrame\n",
    "    submission_df = pd.DataFrame({\n",
    "        \"id\": test_df[\"id\"],  # or whatever your ID is\n",
    "        \"anger_pred\":    test_predictions[:, 0],\n",
    "        \"disgust_pred\":  test_predictions[:, 1],\n",
    "        \"fear_pred\":     test_predictions[:, 2],\n",
    "        \"joy_pred\":      test_predictions[:, 3],\n",
    "        \"sadness_pred\": test_predictions[:, 4],\n",
    "        \"surprise_pred\":test_predictions[:, 5]\n",
    "    })\n",
    "\n",
    "    submission_df.to_csv(submission_file_path, index=False)\n",
    "    print(f\"Submission file saved to {submission_file_path}\")\n",
    "    return submission_df\n",
    "\n",
    "submission_file_path = \"submission.csv\"\n",
    "submission_df = predict_and_generate_submission(test_loader, best_model_path, submission_file_path)\n",
    "submission_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tla8NpESSvNV",
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6443852,
     "sourceId": 10399957,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6444772,
     "sourceId": 10401145,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6457498,
     "sourceId": 10419045,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6558244,
     "sourceId": 10595764,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6558279,
     "sourceId": 10595813,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6558386,
     "sourceId": 10595949,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30840,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

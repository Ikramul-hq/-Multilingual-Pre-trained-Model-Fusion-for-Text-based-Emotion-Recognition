{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10499860,"sourceType":"datasetVersion","datasetId":6500972},{"sourceId":10500046,"sourceType":"datasetVersion","datasetId":6501080}],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset, TensorDataset\nfrom sklearn.model_selection import train_test_split\nfrom transformers import AutoModel, AutoTokenizer\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support\nfrom tqdm import tqdm\nimport random\nimport os","metadata":{"id":"6ZxMbO-2xkoX","trusted":true,"execution":{"iopub.status.busy":"2025-01-22T04:17:20.967693Z","iopub.execute_input":"2025-01-22T04:17:20.968010Z","iopub.status.idle":"2025-01-22T04:17:28.850469Z","shell.execute_reply.started":"2025-01-22T04:17:20.967980Z","shell.execute_reply":"2025-01-22T04:17:28.849831Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"df_main = pd.read_csv(\"/content/eng.csv\")  # just as example\n\ntrain-test split (for simplicity we won't do stratify here for multi-label)\ntrain_df, val_df = train_test_split(df_main, test_size=0.1, random_state=42)","metadata":{"id":"H8a28IMCo0c1"}},{"cell_type":"code","source":"train_df = pd.read_csv(\"/kaggle/input/datasetsemi/eng.csv\")\ntrain_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T04:17:28.851481Z","iopub.execute_input":"2025-01-22T04:17:28.851939Z","iopub.status.idle":"2025-01-22T04:17:28.890790Z","shell.execute_reply.started":"2025-01-22T04:17:28.851906Z","shell.execute_reply":"2025-01-22T04:17:28.890203Z"}},"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"                        id                                               text  \\\n0  eng_train_track_a_00001                       Colorado, middle of nowhere.   \n1  eng_train_track_a_00002  This involved swimming a pretty large lake tha...   \n2  eng_train_track_a_00003        It was one of my most shameful experiences.   \n3  eng_train_track_a_00004  After all, I had vegetables coming out my ears...   \n4  eng_train_track_a_00005                        Then the screaming started.   \n\n   anger  fear  joy  sadness  surprise  \n0      0     1    0        0         1  \n1      0     1    0        0         0  \n2      0     1    0        1         0  \n3      0     0    0        0         0  \n4      0     1    0        1         1  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>text</th>\n      <th>anger</th>\n      <th>fear</th>\n      <th>joy</th>\n      <th>sadness</th>\n      <th>surprise</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>eng_train_track_a_00001</td>\n      <td>Colorado, middle of nowhere.</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>eng_train_track_a_00002</td>\n      <td>This involved swimming a pretty large lake tha...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>eng_train_track_a_00003</td>\n      <td>It was one of my most shameful experiences.</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>eng_train_track_a_00004</td>\n      <td>After all, I had vegetables coming out my ears...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>eng_train_track_a_00005</td>\n      <td>Then the screaming started.</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"val_df = pd.read_csv(\"/kaggle/input/datasetsemi/engdev.csv\")\nval_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T04:17:28.891998Z","iopub.execute_input":"2025-01-22T04:17:28.892229Z","iopub.status.idle":"2025-01-22T04:17:28.905907Z","shell.execute_reply.started":"2025-01-22T04:17:28.892209Z","shell.execute_reply":"2025-01-22T04:17:28.905178Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"                      id                                               text  \\\n0  eng_dev_track_a_00001  Older sister (23 at the time) is a Scumbag Stacy.   \n1  eng_dev_track_a_00002  And I laughed like this: garhahagar, because m...   \n2  eng_dev_track_a_00003  It overflowed and brown shitty diarrhea water ...   \n3  eng_dev_track_a_00004                           Its very dark and foggy.   \n4  eng_dev_track_a_00005  Then she tried to, like, have sex with/strangl...   \n\n   anger  fear  joy  sadness  surprise  \n0      1     0    0        0         0  \n1      0     1    0        0         0  \n2      1     1    0        1         1  \n3      0     1    0        0         0  \n4      1     1    0        0         1  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>text</th>\n      <th>anger</th>\n      <th>fear</th>\n      <th>joy</th>\n      <th>sadness</th>\n      <th>surprise</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>eng_dev_track_a_00001</td>\n      <td>Older sister (23 at the time) is a Scumbag Stacy.</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>eng_dev_track_a_00002</td>\n      <td>And I laughed like this: garhahagar, because m...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>eng_dev_track_a_00003</td>\n      <td>It overflowed and brown shitty diarrhea water ...</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>eng_dev_track_a_00004</td>\n      <td>Its very dark and foggy.</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>eng_dev_track_a_00005</td>\n      <td>Then she tried to, like, have sex with/strangl...</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"test_df = pd.read_csv(\"/kaggle/input/datasetsemi/engtest.csv\")\ntest_df.head()","metadata":{"id":"WGEqnfUJLCik","trusted":true,"execution":{"iopub.status.busy":"2025-01-22T04:17:28.906952Z","iopub.execute_input":"2025-01-22T04:17:28.907254Z","iopub.status.idle":"2025-01-22T04:17:28.933735Z","shell.execute_reply.started":"2025-01-22T04:17:28.907232Z","shell.execute_reply":"2025-01-22T04:17:28.933030Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/pandas/io/formats/format.py:1458: RuntimeWarning: invalid value encountered in greater\n  has_large_values = (abs_vals > 1e6).any()\n/usr/local/lib/python3.10/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in less\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n/usr/local/lib/python3.10/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in greater\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n","output_type":"stream"},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"                       id                                               text  \\\n0  eng_test_track_a_00001  / o \\ So today I went in for a new exam with D...   \n1  eng_test_track_a_00002  The image I have in my mind is this: a group o...   \n2  eng_test_track_a_00003  I slammed my fist against the door and yelled,...   \n3  eng_test_track_a_00004                       I could not unbend my knees.   \n4  eng_test_track_a_00005  I spent the night at the hotel, mostly hanging...   \n\n   anger  fear  joy  sadness  surprise  \n0    NaN   NaN  NaN      NaN       NaN  \n1    NaN   NaN  NaN      NaN       NaN  \n2    NaN   NaN  NaN      NaN       NaN  \n3    NaN   NaN  NaN      NaN       NaN  \n4    NaN   NaN  NaN      NaN       NaN  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>text</th>\n      <th>anger</th>\n      <th>fear</th>\n      <th>joy</th>\n      <th>sadness</th>\n      <th>surprise</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>eng_test_track_a_00001</td>\n      <td>/ o \\ So today I went in for a new exam with D...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>eng_test_track_a_00002</td>\n      <td>The image I have in my mind is this: a group o...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>eng_test_track_a_00003</td>\n      <td>I slammed my fist against the door and yelled,...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>eng_test_track_a_00004</td>\n      <td>I could not unbend my knees.</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>eng_test_track_a_00005</td>\n      <td>I spent the night at the hotel, mostly hanging...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":4},{"cell_type":"markdown","source":"test_text_embeddings = extract_text_embeddings(\n    df=test_df,\n    save_path=\"test_text_embeddings.pt\",  # or another path\n    model=text_model,\n    tokenizer=text_tokenizer,\n    max_length=max_length\n)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AiZJG2u-RQJR","outputId":"36611c2a-6534-4c16-f903-83b762f3d2db","execution":{"iopub.status.busy":"2025-01-17T19:05:50.489409Z","iopub.execute_input":"2025-01-17T19:05:50.489741Z","iopub.status.idle":"2025-01-17T19:05:50.506280Z","shell.execute_reply.started":"2025-01-17T19:05:50.489714Z","shell.execute_reply":"2025-01-17T19:05:50.504974Z"}}},{"cell_type":"markdown","source":"# We do NOT have labels in the test set, so pass has_labels=False\nX_test = prepare_text_embeddings(\n    text_embeddings=test_text_embeddings,\n    df=test_df,\n    has_labels=False\n)\n\n# Create a TensorDataset from X_test\ntest_dataset = TensorDataset(X_test)\n\n# Create a DataLoader from that dataset\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)","metadata":{"id":"AJgiIwu4RbaB","execution":{"iopub.status.busy":"2025-01-17T19:06:05.193068Z","iopub.execute_input":"2025-01-17T19:06:05.193375Z","iopub.status.idle":"2025-01-17T19:06:05.209937Z","shell.execute_reply.started":"2025-01-17T19:06:05.193349Z","shell.execute_reply":"2025-01-17T19:06:05.208683Z"}}},{"cell_type":"code","source":"#df_main.head()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"I36GiCWsyFiV","outputId":"307e438d-8d57-45bc-9e78-b7e639483a66","trusted":true,"execution":{"iopub.status.busy":"2025-01-22T04:17:28.934412Z","iopub.execute_input":"2025-01-22T04:17:28.934653Z","iopub.status.idle":"2025-01-22T04:17:28.937905Z","shell.execute_reply.started":"2025-01-22T04:17:28.934632Z","shell.execute_reply":"2025-01-22T04:17:28.937044Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nseed = 42\ntorch.manual_seed(seed)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5HWiW6WiyH6A","outputId":"6144e19b-133a-4a7f-90c6-504f270401b7","trusted":true,"execution":{"iopub.status.busy":"2025-01-22T04:17:28.938725Z","iopub.execute_input":"2025-01-22T04:17:28.938994Z","iopub.status.idle":"2025-01-22T04:17:29.029201Z","shell.execute_reply.started":"2025-01-22T04:17:28.938966Z","shell.execute_reply":"2025-01-22T04:17:29.027892Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"<torch._C.Generator at 0x7fdf5d5594b0>"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"model_name = \"j-hartmann/emotion-english-distilroberta-base\"  # or any model of your choice\nbatch_size = 8\nmax_length = 256  # or 512, etc.","metadata":{"id":"owCdf45xyXxm","trusted":true,"execution":{"iopub.status.busy":"2025-01-22T04:17:29.030541Z","iopub.execute_input":"2025-01-22T04:17:29.030915Z","iopub.status.idle":"2025-01-22T04:17:29.035597Z","shell.execute_reply.started":"2025-01-22T04:17:29.030877Z","shell.execute_reply":"2025-01-22T04:17:29.034505Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"num_labels = 5","metadata":{"id":"3e4SeV1WzXZ0","trusted":true,"execution":{"iopub.status.busy":"2025-01-22T04:17:29.038812Z","iopub.execute_input":"2025-01-22T04:17:29.039076Z","iopub.status.idle":"2025-01-22T04:17:29.047191Z","shell.execute_reply.started":"2025-01-22T04:17:29.039047Z","shell.execute_reply":"2025-01-22T04:17:29.046096Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"text_tokenizer = AutoTokenizer.from_pretrained(model_name)\ntext_model = AutoModel.from_pretrained(model_name).to(device)","metadata":{"id":"Ed0miVmRzYpd","trusted":true,"execution":{"iopub.status.busy":"2025-01-22T04:17:29.049382Z","iopub.execute_input":"2025-01-22T04:17:29.049619Z","iopub.status.idle":"2025-01-22T04:17:44.547090Z","shell.execute_reply.started":"2025-01-22T04:17:29.049599Z","shell.execute_reply":"2025-01-22T04:17:44.545875Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/294 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cc9f40ff050d481397a37ad9b76095c0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.00k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d083f40748b443d4b9e3fcdb24c08663"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"13f1fba4a123432eb91c6dea4d0b8814"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2b3b98de3979484b9cfd0cdf5b0712e3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2175214ec4764ad785423db86252bdfb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c929ea5bb4b04062b66f309473077802"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/329M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7b01f4c656944cf1b0dc03f59e47c5a5"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaModel were not initialized from the model checkpoint at j-hartmann/emotion-english-distilroberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"class TextDataset(Dataset):\n    \"\"\"\n    Returns two things:\n      1) Embedded text (or tokenized text) for each row\n      2) Multi-label tensor of shape (6,) for [anger, disgust, fear, joy, sadness, surprise]\n    \"\"\"\n    def __init__(self, df, tokenizer, max_length, is_train=True):\n        self.df = df.reset_index(drop=True)\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        self.is_train = is_train\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        text = row[\"text\"] if isinstance(row[\"text\"], str) else \"\"\n\n        # Tokenize text\n        inputs = self.tokenizer(\n            text,\n            padding=\"max_length\",\n            truncation=True,\n            max_length=self.max_length,\n            return_tensors=\"pt\"\n        )\n\n        # We want (batch, seq) shape, so we’ll squeeze out the 1st dimension\n        # inputs[\"input_ids\"], inputs[\"attention_mask\"], ...\n        for k, v in inputs.items():\n            inputs[k] = v.squeeze(0)\n\n        # If train or val, get the multi-label vector\n        if self.is_train:\n            # Convert [anger, disgust, fear, joy, sadness, surprise] to a torch tensor\n            labels = torch.tensor([\n                row[\"anger\"],\n                row[\"fear\"],\n                row[\"joy\"],\n                row[\"sadness\"],\n                row[\"surprise\"]\n            ], dtype=torch.float)  # float for BCEWithLogitsLoss\n            return inputs, labels\n        else:\n            return inputs\n","metadata":{"id":"f11AIUIgzcNM","trusted":true,"execution":{"iopub.status.busy":"2025-01-22T04:17:44.548732Z","iopub.execute_input":"2025-01-22T04:17:44.550493Z","iopub.status.idle":"2025-01-22T04:17:44.558170Z","shell.execute_reply.started":"2025-01-22T04:17:44.550459Z","shell.execute_reply":"2025-01-22T04:17:44.557290Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# 5. Extract embeddings function\n# ------------------\ndef extract_text_embeddings(df, save_path, model, tokenizer, max_length=128):\n    \"\"\"\n    Convert text in df into [CLS] embeddings from a transformer.\n    This is optional if you want an MLP on top.\n    Or you can do end-to-end fine-tuning.\n    \"\"\"\n\n    if os.path.exists(save_path):\n        print(f\"Embeddings already exist at {save_path}\")\n        return torch.load(save_path)\n\n    model.eval()\n    embeddings = {}\n\n    with torch.no_grad():\n        for idx, row in tqdm(df.iterrows(), desc=\"Extracting embeddings\", total=len(df)):\n            text = row[\"text\"] if isinstance(row[\"text\"], str) else \"\"\n            inputs = tokenizer(\n                text,\n                padding=\"max_length\",\n                truncation=True,\n                max_length=max_length,\n                return_tensors=\"pt\"\n            )\n            inputs = {k: v.to(device) for k, v in inputs.items()}\n            outputs = model(**inputs)\n            cls_embedding = outputs.last_hidden_state[:, 0, :]  # [CLS] embedding\n            embeddings[idx] = cls_embedding.cpu()\n\n    torch.save(embeddings, save_path)\n    return embeddings\n","metadata":{"id":"J9tcDuZCzmJv","trusted":true,"execution":{"iopub.status.busy":"2025-01-22T04:17:44.559144Z","iopub.execute_input":"2025-01-22T04:17:44.559454Z","iopub.status.idle":"2025-01-22T04:17:44.573466Z","shell.execute_reply.started":"2025-01-22T04:17:44.559425Z","shell.execute_reply":"2025-01-22T04:17:44.572679Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# (Optional) Actually do it for train, val\ntrain_text_embeddings = extract_text_embeddings(\n    train_df, \"train_text_embeddings.pt\", text_model, text_tokenizer, max_length\n)\nval_text_embeddings = extract_text_embeddings(\n    val_df, \"val_text_embeddings.pt\", text_model, text_tokenizer, max_length\n)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"53AZjn4Lztx_","outputId":"db18509d-71da-46cd-e49c-a5be034bdf6e","trusted":true,"execution":{"iopub.status.busy":"2025-01-22T04:17:44.574644Z","iopub.execute_input":"2025-01-22T04:17:44.574925Z","iopub.status.idle":"2025-01-22T04:18:08.480607Z","shell.execute_reply.started":"2025-01-22T04:17:44.574897Z","shell.execute_reply":"2025-01-22T04:18:08.479850Z"}},"outputs":[{"name":"stderr","text":"Extracting embeddings: 100%|██████████| 2768/2768 [00:22<00:00, 121.29it/s]\nExtracting embeddings: 100%|██████████| 116/116 [00:00<00:00, 123.33it/s]\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"test_text_embeddings = extract_text_embeddings(\n    df=test_df,\n    save_path=\"test_text_embeddings.pt\",  # or another path\n    model=text_model,\n    tokenizer=text_tokenizer,\n    max_length=max_length\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T04:18:08.481364Z","iopub.execute_input":"2025-01-22T04:18:08.481622Z","iopub.status.idle":"2025-01-22T04:18:32.010952Z","shell.execute_reply.started":"2025-01-22T04:18:08.481589Z","shell.execute_reply":"2025-01-22T04:18:32.010068Z"}},"outputs":[{"name":"stderr","text":"Extracting embeddings: 100%|██████████| 2767/2767 [00:23<00:00, 118.22it/s]\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# 6. Convert embeddings + labels into Tensors\n# ------------------\ndef prepare_text_embeddings(text_embeddings, df, has_labels=True):\n    \"\"\" Stack all embeddings into a single tensor, and the 6‐dim label if available. \"\"\"\n    combined_embeddings = []\n    combined_labels = []\n\n    for idx, row in df.iterrows():\n        if idx not in text_embeddings:\n            continue\n        emb = text_embeddings[idx].squeeze(0)\n        combined_embeddings.append(emb)\n\n        if has_labels:\n            labs = [\n                row[\"anger\"],\n                row[\"fear\"],\n                row[\"joy\"],\n                row[\"sadness\"],\n                row[\"surprise\"]\n            ]\n            combined_labels.append(labs)\n\n    X = torch.stack(combined_embeddings)\n    if has_labels:\n        y = torch.tensor(combined_labels, dtype=torch.float)\n        return X, y\n    else:\n        return X\n\nX_train, y_train = prepare_text_embeddings(train_text_embeddings, train_df, has_labels=True)\nX_val, y_val = prepare_text_embeddings(val_text_embeddings, val_df, has_labels=True)\n\nprint(\"Shapes:\", X_train.shape, y_train.shape, \"|\", X_val.shape, y_val.shape)\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l-rDsNzpz429","outputId":"9c9cb96a-23d7-4d35-ac57-43cd38462198","trusted":true,"execution":{"iopub.status.busy":"2025-01-22T04:18:32.012080Z","iopub.execute_input":"2025-01-22T04:18:32.012424Z","iopub.status.idle":"2025-01-22T04:18:32.210009Z","shell.execute_reply.started":"2025-01-22T04:18:32.012393Z","shell.execute_reply":"2025-01-22T04:18:32.209174Z"}},"outputs":[{"name":"stdout","text":"Shapes: torch.Size([2768, 768]) torch.Size([2768, 5]) | torch.Size([116, 768]) torch.Size([116, 5])\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 7. Define a Multi-label MLP model\n# ------------------\nclass MLPModel(nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim, dropout_p=0.5):\n        super(MLPModel, self).__init__()\n        self.fc1 = nn.Linear(input_dim, hidden_dim[0])\n        self.relu = nn.ReLU()\n        self.dropout1 = nn.Dropout(p=dropout_p)\n        self.fc2 = nn.Linear(hidden_dim[0], hidden_dim[1])\n        self.dropout2 = nn.Dropout(p=dropout_p)\n        self.fc3 = nn.Linear(hidden_dim[1], output_dim)\n\n    def forward(self, x):\n        x = self.relu(self.fc1(x))\n        x = self.dropout1(x)\n        x = self.relu(self.fc2(x))\n        x = self.dropout2(x)\n        # For multi‐label, we do not apply sigmoid here.\n        # We'll use BCEWithLogitsLoss, which combines sigmoid + BCE in a stable way.\n        x = self.fc3(x)\n        return x\n","metadata":{"id":"d6VBLMJ-z8vX","trusted":true,"execution":{"iopub.status.busy":"2025-01-22T04:18:32.210970Z","iopub.execute_input":"2025-01-22T04:18:32.211295Z","iopub.status.idle":"2025-01-22T04:18:32.216761Z","shell.execute_reply.started":"2025-01-22T04:18:32.211263Z","shell.execute_reply":"2025-01-22T04:18:32.215891Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# 8. Prepare Data Loaders\n# ------------------\ntrain_dataset = TensorDataset(X_train, y_train)\nval_dataset   = TensorDataset(X_val, y_val)\n\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_loader   = DataLoader(val_dataset,   batch_size=batch_size, shuffle=False)\n","metadata":{"id":"VRAeU8nq0Arm","trusted":true,"execution":{"iopub.status.busy":"2025-01-22T04:18:32.217713Z","iopub.execute_input":"2025-01-22T04:18:32.217968Z","iopub.status.idle":"2025-01-22T04:18:32.230541Z","shell.execute_reply.started":"2025-01-22T04:18:32.217948Z","shell.execute_reply":"2025-01-22T04:18:32.229716Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"test_text_embeddings = extract_text_embeddings(\n    df=test_df,\n    save_path=\"test_text_embeddings.pt\",  # or another path\n    model=text_model,\n    tokenizer=text_tokenizer,\n    max_length=max_length\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T04:18:32.231491Z","iopub.execute_input":"2025-01-22T04:18:32.231715Z","iopub.status.idle":"2025-01-22T04:18:32.367039Z","shell.execute_reply.started":"2025-01-22T04:18:32.231697Z","shell.execute_reply":"2025-01-22T04:18:32.366206Z"}},"outputs":[{"name":"stdout","text":"Embeddings already exist at test_text_embeddings.pt\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-11-4af6ac68290f>:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  return torch.load(save_path)\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"# We do NOT have labels in the test set, so pass has_labels=False\nX_test = prepare_text_embeddings(\n    text_embeddings=test_text_embeddings,\n    df=test_df,\n    has_labels=False\n)\n\n# Create a TensorDataset from X_test\ntest_dataset = TensorDataset(X_test)\n\n# Create a DataLoader from that dataset\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-22T04:18:32.368128Z","iopub.execute_input":"2025-01-22T04:18:32.368348Z","iopub.status.idle":"2025-01-22T04:18:32.490469Z","shell.execute_reply.started":"2025-01-22T04:18:32.368330Z","shell.execute_reply":"2025-01-22T04:18:32.489859Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"# 9. Training Setup\n# ------------------\ninput_dim   = X_train.shape[1]  # e.g. 768 if your transformer is a base model\nhidden_dim  = [1024, 512]       # can adjust\noutput_dim  = num_labels        # 6 for [anger, disgust, fear, joy, sadness, surprise]\ndropout_p   = 0.3\nnum_epochs  = 50\nlearning_rate_options = [0.0005, 0.001, 0.002]\nlearning_rate = random.choice(learning_rate_options)\n","metadata":{"id":"NxoLdLcB0EQv","trusted":true,"execution":{"iopub.status.busy":"2025-01-22T04:18:32.491147Z","iopub.execute_input":"2025-01-22T04:18:32.491331Z","iopub.status.idle":"2025-01-22T04:18:32.495307Z","shell.execute_reply.started":"2025-01-22T04:18:32.491315Z","shell.execute_reply":"2025-01-22T04:18:32.494488Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"model = MLPModel(input_dim, hidden_dim, output_dim, dropout_p).to(device)\n# For multi-label classification => BCEWithLogitsLoss\ncriterion = nn.BCEWithLogitsLoss()\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)","metadata":{"id":"C7kUvJri04QK","trusted":true,"execution":{"iopub.status.busy":"2025-01-22T04:18:32.496086Z","iopub.execute_input":"2025-01-22T04:18:32.496385Z","iopub.status.idle":"2025-01-22T04:18:32.523809Z","shell.execute_reply.started":"2025-01-22T04:18:32.496352Z","shell.execute_reply":"2025-01-22T04:18:32.523029Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"# 10. Training Loop\n# ------------------\ndef train_one_epoch(model, loader, optimizer, criterion):\n    model.train()\n    total_loss = 0\n    all_preds = []\n    all_labels = []\n\n    for X_batch, y_batch in loader:\n        X_batch = X_batch.to(device)\n        y_batch = y_batch.to(device)  # shape (batch, 6)\n\n        optimizer.zero_grad()\n        logits = model(X_batch)  # shape (batch, 6)\n        loss = criterion(logits, y_batch)\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n\n        # Convert to predictions\n        preds = torch.sigmoid(logits)  # apply sigmoid\n        preds = (preds >= 0.5).float() # threshold\n        all_preds.append(preds.detach().cpu())\n        all_labels.append(y_batch.detach().cpu())\n\n    avg_loss = total_loss / len(loader)\n    all_preds  = torch.cat(all_preds, dim=0)\n    all_labels = torch.cat(all_labels, dim=0)\n    return avg_loss, all_preds, all_labels","metadata":{"id":"f1D5DBXj05vT","trusted":true,"execution":{"iopub.status.busy":"2025-01-22T04:18:32.524660Z","iopub.execute_input":"2025-01-22T04:18:32.524937Z","iopub.status.idle":"2025-01-22T04:18:32.530402Z","shell.execute_reply.started":"2025-01-22T04:18:32.524908Z","shell.execute_reply":"2025-01-22T04:18:32.529709Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"def evaluate(model, loader, criterion):\n    model.eval()\n    total_loss = 0\n    all_preds = []\n    all_labels = []\n\n    with torch.no_grad():\n        for X_batch, y_batch in loader:\n            X_batch = X_batch.to(device)\n            y_batch = y_batch.to(device)\n\n            logits = model(X_batch)\n            loss = criterion(logits, y_batch)\n            total_loss += loss.item()\n\n            preds = torch.sigmoid(logits)\n            preds = (preds >= 0.5).float()\n            all_preds.append(preds.detach().cpu())\n            all_labels.append(y_batch.detach().cpu())\n\n    avg_loss = total_loss / len(loader)\n    all_preds  = torch.cat(all_preds, dim=0)\n    all_labels = torch.cat(all_labels, dim=0)\n    return avg_loss, all_preds, all_labels\n","metadata":{"id":"IwuEut3r1ggL","trusted":true,"execution":{"iopub.status.busy":"2025-01-22T04:18:32.531209Z","iopub.execute_input":"2025-01-22T04:18:32.531430Z","iopub.status.idle":"2025-01-22T04:18:32.543755Z","shell.execute_reply.started":"2025-01-22T04:18:32.531411Z","shell.execute_reply":"2025-01-22T04:18:32.543065Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"\nfrom sklearn.metrics import precision_recall_fscore_support\n\ndef calculate_metrics(preds, labels):\n    \"\"\"\n    Multi-label version of the metrics:\n      - preds, labels: shape (batch_size, 5) with 0/1 values.\n      - We compute:\n         * Subset accuracy: fraction of samples with exactly matching labels\n         * Macro Precision, Recall, F1\n    \"\"\"\n    # Convert to CPU numpy if needed\n    preds_np = preds.cpu().numpy()\n    labels_np = labels.cpu().numpy()\n\n    # Subset accuracy: the sample is correct only if *all* 5 labels match\n    exact_matches = (preds_np == labels_np).all(axis=1)\n    subset_accuracy = exact_matches.mean()\n\n    # Macro-averaged P, R, F1 across the 5 labels\n    p, r, f, _ = precision_recall_fscore_support(\n        labels_np,\n        preds_np,\n        average=\"macro\",\n        zero_division=0\n    )\n    return subset_accuracy, p, r, f","metadata":{"id":"MXspdqJ-1j0l","trusted":true,"execution":{"iopub.status.busy":"2025-01-22T04:18:32.546644Z","iopub.execute_input":"2025-01-22T04:18:32.546840Z","iopub.status.idle":"2025-01-22T04:18:32.554310Z","shell.execute_reply.started":"2025-01-22T04:18:32.546823Z","shell.execute_reply":"2025-01-22T04:18:32.553647Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"def train_and_save_best_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, save_dir):\n    best_f1 = -float('inf')\n    best_model_path = None\n\n    for epoch in range(num_epochs):\n        ######################\n        # 1) Training Phase\n        ######################\n        model.train()\n        train_loss = 0.0\n        all_train_preds, all_train_labels = [], []\n\n        for inputs, labels in train_loader:\n            inputs, labels = inputs.to(device), labels.to(device)  # shape (batch, 768), (batch, 5)\n\n            optimizer.zero_grad()\n            outputs = model(inputs).squeeze()  # shape (batch, 5)\n            loss = criterion(outputs, labels)  # BCEWithLogitsLoss\n            loss.backward()\n            optimizer.step()\n\n            train_loss += loss.item()\n\n            # Convert logits -> probabilities -> 0/1 predictions\n            probs = torch.sigmoid(outputs)\n            preds = (probs >= 0.5).float()\n            all_train_preds.append(preds)\n            all_train_labels.append(labels)\n\n        avg_train_loss = train_loss / len(train_loader)\n        all_train_preds = torch.cat(all_train_preds, dim=0)\n        all_train_labels = torch.cat(all_train_labels, dim=0)\n\n        # Calculate multi-label metrics for training set\n        train_accuracy, train_precision, train_recall, train_f1 = calculate_metrics(all_train_preds, all_train_labels)\n\n        ######################\n        # 2) Validation Phase\n        ######################\n        model.eval()\n        val_loss = 0.0\n        all_val_preds, all_val_labels = [], []\n\n        with torch.no_grad():\n            for inputs, labels in val_loader:\n                inputs, labels = inputs.to(device), labels.to(device)\n                outputs = model(inputs).squeeze()\n                loss = criterion(outputs, labels)\n                val_loss += loss.item()\n\n                probs = torch.sigmoid(outputs)\n                preds = (probs >= 0.5).float()\n                all_val_preds.append(preds)\n                all_val_labels.append(labels)\n\n        avg_val_loss = val_loss / len(val_loader)\n        all_val_preds = torch.cat(all_val_preds, dim=0)\n        all_val_labels = torch.cat(all_val_labels, dim=0)\n\n        # Calculate multi-label metrics for validation set\n        val_accuracy, val_precision, val_recall, val_f1 = calculate_metrics(all_val_preds, all_val_labels)\n\n        print(f\"Epoch {epoch+1}/{num_epochs}: \"\n              f\"Train Loss: {avg_train_loss:.4f}, \"\n              f\"Train Acc: {train_accuracy:.4f}, Prec: {train_precision:.4f}, Rec: {train_recall:.4f}, F1: {train_f1:.4f} | \"\n              f\"Val Loss: {avg_val_loss:.4f}, Val Acc: {val_accuracy:.4f}, \"\n              f\"Prec: {val_precision:.4f}, Rec: {val_recall:.4f}, F1: {val_f1:.4f}\")\n\n        # Save the model if it has the best F1 score on validation\n        if val_f1 > best_f1:\n            best_f1 = val_f1\n            best_model_path = f\"{save_dir}/best_model_epoch_{epoch + 1}_f1_{val_f1:.4f}.pth\"\n            torch.save(model.state_dict(), best_model_path)\n            print(f\"Best model saved with F1: {val_f1:.4f} at epoch {epoch + 1}\")\n\n            # For your Excel logging\n            a = round(train_accuracy,4)\n            b = round(train_precision,4)\n            c = round(train_recall,4)\n            d = round(train_f1,4)\n            e = round(val_accuracy,4)\n            f_ = round(val_precision,4)\n            g = round(val_recall,4)\n            h = round(val_f1,4)\n\n    # Return best path plus the last known metrics\n    return best_model_path, a, b, c, d, e, f_, g, h","metadata":{"id":"3ZiQjjDZEIBo","trusted":true,"execution":{"iopub.status.busy":"2025-01-22T04:18:32.555231Z","iopub.execute_input":"2025-01-22T04:18:32.555488Z","iopub.status.idle":"2025-01-22T04:18:32.569582Z","shell.execute_reply.started":"2025-01-22T04:18:32.555459Z","shell.execute_reply":"2025-01-22T04:18:32.568932Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"#  RUN and SAVE\n# ------------------\nsave_dir = \"./models\"\nos.makedirs(save_dir, exist_ok=True)\n\nbest_model_path, a, b, c, d, e, f_, g, h = train_and_save_best_model(\n    model=model,\n    train_loader=train_loader,\n    val_loader=val_loader,\n    criterion=criterion,\n    optimizer=optimizer,\n    num_epochs=num_epochs,\n    save_dir=save_dir\n)\n\nprint(f\"Best model saved at: {best_model_path}\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"J6yIeYEV2lIv","outputId":"ca2a48d4-79e6-4fec-9d85-6f436b9383a9","trusted":true,"execution":{"iopub.status.busy":"2025-01-22T04:18:32.570333Z","iopub.execute_input":"2025-01-22T04:18:32.570602Z","iopub.status.idle":"2025-01-22T04:19:10.889539Z","shell.execute_reply.started":"2025-01-22T04:18:32.570574Z","shell.execute_reply":"2025-01-22T04:19:10.888812Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/50: Train Loss: 0.4686, Train Acc: 0.3118, Prec: 0.6541, Rec: 0.4812, F1: 0.5405 | Val Loss: 0.4657, Val Acc: 0.3103, Prec: 0.7151, Rec: 0.5023, F1: 0.5601\nBest model saved with F1: 0.5601 at epoch 1\nEpoch 2/50: Train Loss: 0.4373, Train Acc: 0.3436, Prec: 0.6890, Rec: 0.5352, F1: 0.5935 | Val Loss: 0.4276, Val Acc: 0.3017, Prec: 0.7698, Rec: 0.4586, F1: 0.5575\nEpoch 3/50: Train Loss: 0.4144, Train Acc: 0.3587, Prec: 0.6930, Rec: 0.5516, F1: 0.6079 | Val Loss: 0.4322, Val Acc: 0.3621, Prec: 0.7213, Rec: 0.5631, F1: 0.6129\nBest model saved with F1: 0.6129 at epoch 3\nEpoch 4/50: Train Loss: 0.3968, Train Acc: 0.3855, Prec: 0.7065, Rec: 0.5802, F1: 0.6303 | Val Loss: 0.4459, Val Acc: 0.3621, Prec: 0.7315, Rec: 0.5285, F1: 0.5984\nEpoch 5/50: Train Loss: 0.3808, Train Acc: 0.4090, Prec: 0.7179, Rec: 0.6038, F1: 0.6507 | Val Loss: 0.4539, Val Acc: 0.3362, Prec: 0.7476, Rec: 0.5414, F1: 0.5922\nEpoch 6/50: Train Loss: 0.3636, Train Acc: 0.4277, Prec: 0.7287, Rec: 0.6237, F1: 0.6658 | Val Loss: 0.4581, Val Acc: 0.3276, Prec: 0.7136, Rec: 0.5746, F1: 0.6193\nBest model saved with F1: 0.6193 at epoch 6\nEpoch 7/50: Train Loss: 0.3434, Train Acc: 0.4545, Prec: 0.7504, Rec: 0.6545, F1: 0.6956 | Val Loss: 0.4683, Val Acc: 0.2931, Prec: 0.6827, Rec: 0.4972, F1: 0.5418\nEpoch 8/50: Train Loss: 0.3214, Train Acc: 0.4884, Prec: 0.7672, Rec: 0.6809, F1: 0.7185 | Val Loss: 0.4875, Val Acc: 0.3017, Prec: 0.7518, Rec: 0.5440, F1: 0.5922\nEpoch 9/50: Train Loss: 0.3124, Train Acc: 0.5058, Prec: 0.7849, Rec: 0.6822, F1: 0.7257 | Val Loss: 0.4470, Val Acc: 0.3534, Prec: 0.7063, Rec: 0.6209, F1: 0.6564\nBest model saved with F1: 0.6564 at epoch 9\nEpoch 10/50: Train Loss: 0.2871, Train Acc: 0.5202, Prec: 0.7855, Rec: 0.7194, F1: 0.7490 | Val Loss: 0.5193, Val Acc: 0.2500, Prec: 0.7145, Rec: 0.5358, F1: 0.5897\nEpoch 11/50: Train Loss: 0.2698, Train Acc: 0.5592, Prec: 0.8045, Rec: 0.7440, F1: 0.7713 | Val Loss: 0.5147, Val Acc: 0.3621, Prec: 0.7274, Rec: 0.5641, F1: 0.6074\nEpoch 12/50: Train Loss: 0.2496, Train Acc: 0.5863, Prec: 0.8208, Rec: 0.7658, F1: 0.7909 | Val Loss: 0.5412, Val Acc: 0.3103, Prec: 0.6834, Rec: 0.5187, F1: 0.5887\nEpoch 13/50: Train Loss: 0.2277, Train Acc: 0.6189, Prec: 0.8447, Rec: 0.7852, F1: 0.8122 | Val Loss: 0.5403, Val Acc: 0.3103, Prec: 0.6366, Rec: 0.6166, F1: 0.6221\nEpoch 14/50: Train Loss: 0.2249, Train Acc: 0.6319, Prec: 0.8335, Rec: 0.7965, F1: 0.8140 | Val Loss: 0.5439, Val Acc: 0.4052, Prec: 0.7107, Rec: 0.6290, F1: 0.6584\nBest model saved with F1: 0.6584 at epoch 14\nEpoch 15/50: Train Loss: 0.1989, Train Acc: 0.6622, Prec: 0.8503, Rec: 0.8229, F1: 0.8360 | Val Loss: 0.6013, Val Acc: 0.3362, Prec: 0.7055, Rec: 0.5793, F1: 0.6335\nEpoch 16/50: Train Loss: 0.1961, Train Acc: 0.6745, Prec: 0.8615, Rec: 0.8264, F1: 0.8431 | Val Loss: 0.6406, Val Acc: 0.3534, Prec: 0.6700, Rec: 0.5776, F1: 0.6121\nEpoch 17/50: Train Loss: 0.1810, Train Acc: 0.6951, Prec: 0.8704, Rec: 0.8407, F1: 0.8550 | Val Loss: 0.6917, Val Acc: 0.3362, Prec: 0.6711, Rec: 0.6301, F1: 0.6441\nEpoch 18/50: Train Loss: 0.1648, Train Acc: 0.7207, Prec: 0.8885, Rec: 0.8540, F1: 0.8704 | Val Loss: 0.6963, Val Acc: 0.3103, Prec: 0.6379, Rec: 0.6119, F1: 0.6182\nEpoch 19/50: Train Loss: 0.1561, Train Acc: 0.7345, Prec: 0.8905, Rec: 0.8696, F1: 0.8797 | Val Loss: 0.7016, Val Acc: 0.3707, Prec: 0.6493, Rec: 0.6123, F1: 0.6256\nEpoch 20/50: Train Loss: 0.1490, Train Acc: 0.7435, Prec: 0.8934, Rec: 0.8691, F1: 0.8809 | Val Loss: 0.8286, Val Acc: 0.3017, Prec: 0.6560, Rec: 0.5327, F1: 0.5780\nEpoch 21/50: Train Loss: 0.1430, Train Acc: 0.7608, Prec: 0.9031, Rec: 0.8852, F1: 0.8939 | Val Loss: 0.7723, Val Acc: 0.3621, Prec: 0.6992, Rec: 0.5554, F1: 0.6161\nEpoch 22/50: Train Loss: 0.1340, Train Acc: 0.7666, Prec: 0.9001, Rec: 0.8894, F1: 0.8947 | Val Loss: 0.7012, Val Acc: 0.3534, Prec: 0.6711, Rec: 0.6093, F1: 0.6268\nEpoch 23/50: Train Loss: 0.1285, Train Acc: 0.7738, Prec: 0.9125, Rec: 0.8875, F1: 0.8997 | Val Loss: 0.8083, Val Acc: 0.3621, Prec: 0.6906, Rec: 0.6232, F1: 0.6536\nEpoch 24/50: Train Loss: 0.1193, Train Acc: 0.7977, Prec: 0.9178, Rec: 0.9062, F1: 0.9119 | Val Loss: 0.7548, Val Acc: 0.3362, Prec: 0.6781, Rec: 0.5482, F1: 0.6008\nEpoch 25/50: Train Loss: 0.1088, Train Acc: 0.8089, Prec: 0.9255, Rec: 0.9108, F1: 0.9180 | Val Loss: 0.8652, Val Acc: 0.3103, Prec: 0.6217, Rec: 0.5538, F1: 0.5736\nEpoch 26/50: Train Loss: 0.1235, Train Acc: 0.7970, Prec: 0.9155, Rec: 0.9113, F1: 0.9134 | Val Loss: 0.7946, Val Acc: 0.3621, Prec: 0.7163, Rec: 0.6325, F1: 0.6530\nEpoch 27/50: Train Loss: 0.1007, Train Acc: 0.8302, Prec: 0.9306, Rec: 0.9215, F1: 0.9260 | Val Loss: 0.8716, Val Acc: 0.3103, Prec: 0.6871, Rec: 0.5692, F1: 0.6078\nEpoch 28/50: Train Loss: 0.1081, Train Acc: 0.8186, Prec: 0.9251, Rec: 0.9226, F1: 0.9238 | Val Loss: 0.8235, Val Acc: 0.3448, Prec: 0.6551, Rec: 0.5764, F1: 0.6111\nEpoch 29/50: Train Loss: 0.0984, Train Acc: 0.8230, Prec: 0.9267, Rec: 0.9224, F1: 0.9244 | Val Loss: 0.8687, Val Acc: 0.3621, Prec: 0.6911, Rec: 0.5748, F1: 0.6240\nEpoch 30/50: Train Loss: 0.0964, Train Acc: 0.8327, Prec: 0.9368, Rec: 0.9251, F1: 0.9309 | Val Loss: 0.9817, Val Acc: 0.3276, Prec: 0.6654, Rec: 0.6197, F1: 0.6312\nEpoch 31/50: Train Loss: 0.1009, Train Acc: 0.8298, Prec: 0.9297, Rec: 0.9202, F1: 0.9249 | Val Loss: 0.9395, Val Acc: 0.3276, Prec: 0.6633, Rec: 0.5962, F1: 0.6213\nEpoch 32/50: Train Loss: 0.0848, Train Acc: 0.8569, Prec: 0.9411, Rec: 0.9393, F1: 0.9402 | Val Loss: 0.9641, Val Acc: 0.3879, Prec: 0.6909, Rec: 0.5591, F1: 0.6035\nEpoch 33/50: Train Loss: 0.0989, Train Acc: 0.8425, Prec: 0.9335, Rec: 0.9250, F1: 0.9292 | Val Loss: 0.9620, Val Acc: 0.3448, Prec: 0.6711, Rec: 0.6076, F1: 0.6291\nEpoch 34/50: Train Loss: 0.0860, Train Acc: 0.8587, Prec: 0.9493, Rec: 0.9371, F1: 0.9430 | Val Loss: 0.9726, Val Acc: 0.3621, Prec: 0.6694, Rec: 0.6069, F1: 0.6303\nEpoch 35/50: Train Loss: 0.0845, Train Acc: 0.8580, Prec: 0.9416, Rec: 0.9414, F1: 0.9415 | Val Loss: 1.0003, Val Acc: 0.3707, Prec: 0.6775, Rec: 0.6347, F1: 0.6481\nEpoch 36/50: Train Loss: 0.0924, Train Acc: 0.8436, Prec: 0.9348, Rec: 0.9321, F1: 0.9334 | Val Loss: 0.9145, Val Acc: 0.3362, Prec: 0.6779, Rec: 0.5600, F1: 0.6013\nEpoch 37/50: Train Loss: 0.0721, Train Acc: 0.8790, Prec: 0.9543, Rec: 0.9461, F1: 0.9501 | Val Loss: 1.0974, Val Acc: 0.3276, Prec: 0.6943, Rec: 0.5825, F1: 0.6227\nEpoch 38/50: Train Loss: 0.0797, Train Acc: 0.8674, Prec: 0.9476, Rec: 0.9441, F1: 0.9458 | Val Loss: 1.0338, Val Acc: 0.3448, Prec: 0.6240, Rec: 0.6011, F1: 0.6092\nEpoch 39/50: Train Loss: 0.0753, Train Acc: 0.8710, Prec: 0.9506, Rec: 0.9440, F1: 0.9473 | Val Loss: 1.0086, Val Acc: 0.3793, Prec: 0.7191, Rec: 0.6133, F1: 0.6471\nEpoch 40/50: Train Loss: 0.0728, Train Acc: 0.8808, Prec: 0.9510, Rec: 0.9444, F1: 0.9477 | Val Loss: 1.0741, Val Acc: 0.3362, Prec: 0.6605, Rec: 0.5757, F1: 0.6093\nEpoch 41/50: Train Loss: 0.0776, Train Acc: 0.8656, Prec: 0.9459, Rec: 0.9337, F1: 0.9397 | Val Loss: 1.0149, Val Acc: 0.3103, Prec: 0.6596, Rec: 0.5455, F1: 0.5949\nEpoch 42/50: Train Loss: 0.0769, Train Acc: 0.8721, Prec: 0.9490, Rec: 0.9487, F1: 0.9488 | Val Loss: 1.2034, Val Acc: 0.3362, Prec: 0.6269, Rec: 0.5199, F1: 0.5593\nEpoch 43/50: Train Loss: 0.0699, Train Acc: 0.8829, Prec: 0.9599, Rec: 0.9529, F1: 0.9564 | Val Loss: 1.0306, Val Acc: 0.3448, Prec: 0.6728, Rec: 0.5847, F1: 0.6169\nEpoch 44/50: Train Loss: 0.0731, Train Acc: 0.8815, Prec: 0.9500, Rec: 0.9504, F1: 0.9502 | Val Loss: 1.0271, Val Acc: 0.3621, Prec: 0.6362, Rec: 0.5984, F1: 0.6153\nEpoch 45/50: Train Loss: 0.0644, Train Acc: 0.8952, Prec: 0.9599, Rec: 0.9527, F1: 0.9563 | Val Loss: 1.2026, Val Acc: 0.3017, Prec: 0.6176, Rec: 0.5718, F1: 0.5920\nEpoch 46/50: Train Loss: 0.0575, Train Acc: 0.9028, Prec: 0.9620, Rec: 0.9575, F1: 0.9597 | Val Loss: 1.0696, Val Acc: 0.3448, Prec: 0.6434, Rec: 0.5954, F1: 0.6146\nEpoch 47/50: Train Loss: 0.0658, Train Acc: 0.8931, Prec: 0.9543, Rec: 0.9497, F1: 0.9520 | Val Loss: 1.1627, Val Acc: 0.3793, Prec: 0.6455, Rec: 0.6719, F1: 0.6562\nEpoch 48/50: Train Loss: 0.0714, Train Acc: 0.8804, Prec: 0.9554, Rec: 0.9517, F1: 0.9535 | Val Loss: 1.0833, Val Acc: 0.3707, Prec: 0.6916, Rec: 0.6132, F1: 0.6363\nEpoch 49/50: Train Loss: 0.0643, Train Acc: 0.8996, Prec: 0.9588, Rec: 0.9590, F1: 0.9589 | Val Loss: 1.1517, Val Acc: 0.3621, Prec: 0.7143, Rec: 0.5979, F1: 0.6405\nEpoch 50/50: Train Loss: 0.0730, Train Acc: 0.8876, Prec: 0.9515, Rec: 0.9529, F1: 0.9522 | Val Loss: 1.0243, Val Acc: 0.3793, Prec: 0.6886, Rec: 0.6053, F1: 0.6383\nBest model saved at: ./models/best_model_epoch_14_f1_0.6584.pth\n","output_type":"stream"}],"execution_count":25},{"cell_type":"markdown","source":"# EXCEL LOGGING (SAME IDEA AS BEFORE)\n#####################################\n\nimport os\nfrom openpyxl import Workbook, load_workbook\n\n# Path to your Excel file\nexcel_file = \"/content/drive/MyDrive/ML winter Bootcamp/SemiEval11/Book1.xlsx\"\n\n# Load the workbook to append data\nwb = load_workbook(excel_file)\nws = wb.active\n\n# Identify the next empty row\nnext_row = 1\nfor row in ws.iter_rows(min_row=1, max_col=4):\n    if not row[3].value:\n        next_row = row[0].row\n        break\n    next_row += 1\n\n# Write data starting from column C\nws.cell(row=next_row, column=3,  value=seed)\nws.cell(row=next_row, column=4,  value=max_length)\nws.cell(row=next_row, column=5,  value=batch_size)\nws.cell(row=next_row, column=6,  value=num_epochs)\nws.cell(row=next_row, column=7,  value=str(hidden_dim))\nws.cell(row=next_row, column=8,  value=learning_rate)\nws.cell(row=next_row, column=9,  value=dropout_p)\nws.cell(row=next_row, column=10, value=a)   # Train Acc\nws.cell(row=next_row, column=11, value=b)   # Train Prec\nws.cell(row=next_row, column=12, value=c)   # Train Rec\nws.cell(row=next_row, column=13, value=d)   # Train F1\nws.cell(row=next_row, column=14, value=e)   # Val Acc\nws.cell(row=next_row, column=15, value=f_)  # Val Prec\nws.cell(row=next_row, column=16, value=g)   # Val Rec\nws.cell(row=next_row, column=17, value=h)   # Val F1\n\nwb.save(excel_file)","metadata":{"id":"6O1Y-0llFEnX"}},{"cell_type":"markdown","source":"import pandas as pd\ndf = pd.read_excel(excel_file)\nprint(df)\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NwdYhBNQFD84","outputId":"fab63e70-fe24-4ee3-d451-2a0a768d3f6f","execution":{"iopub.status.busy":"2025-01-17T19:13:39.910043Z","iopub.status.idle":"2025-01-17T19:13:39.910363Z","shell.execute_reply":"2025-01-17T19:13:39.910209Z"}}},{"cell_type":"code","source":"def predict_and_generate_submission(\n    test_loader,\n    best_model_path,\n    submission_file_path,\n    text_df,  # The original test_df so we can reference \"id\" / \"text\"\n    input_dim,\n    hidden_dim,\n    output_dim,\n    dropout_p\n):\n    \"\"\"\n    Multi-label inference on test set. The test_loader yields numeric embeddings (X_test).\n    We do MLP forward pass => sigmoid => threshold => build a CSV with predicted labels.\n    \"\"\"\n    # 1. Load best model\n    model = MLPModel(input_dim, hidden_dim, output_dim, dropout_p).to(device)\n    model.load_state_dict(torch.load(best_model_path, weights_only=True))\n    model.eval()\n\n    # For storing predictions in a list of shape (num_samples, 5)\n    all_test_preds = []\n\n    # 2. Iterate over test_loader\n    with torch.no_grad():\n        for (inputs,) in test_loader:\n            # \"inputs\" is shape (batch_size, embedding_dim)\n            inputs = inputs.to(device)\n            logits = model(inputs)              # shape (batch_size, 5)\n            probs = torch.sigmoid(logits)       # shape (batch_size, 5)\n            preds = (probs >= 0.5).float().cpu() # shape (batch_size, 5), 0/1\n            all_test_preds.extend(preds.numpy())\n\n    # 3. Build the submission DataFrame\n    #    Suppose you want the \"id\" and \"text\" columns from the test_df\n    submission_df = pd.DataFrame({\n        \"id\":   text_df[\"id\"].values,\n        \"text\": text_df[\"text\"].values,\n        \"anger\":    [int(row[0]) for row in all_test_preds],\n        \"fear\":     [int(row[1]) for row in all_test_preds],\n        \"joy\":      [int(row[2]) for row in all_test_preds],\n        \"sadness\":  [int(row[3]) for row in all_test_preds],\n        \"surprise\": [int(row[4]) for row in all_test_preds],\n    })\n\n    # 4. Save CSV\n    submission_df.to_csv(submission_file_path, index=False, encoding=\"utf-8\")\n    print(f\"Submission saved to {submission_file_path}\")\n\n    return submission_df\n\n# Example usage:\nsubmission_file_path = \"submission.csv\"\nsubmission_df = predict_and_generate_submission(\n    test_loader=test_loader,\n    best_model_path=best_model_path,\n    submission_file_path=submission_file_path,\n    text_df=test_df,   # the original DataFrame for the test set\n    input_dim=input_dim,\n    hidden_dim=hidden_dim,\n    output_dim=output_dim,\n    dropout_p=dropout_p\n)\n\nsubmission_df.head()\n","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":224},"id":"Bi9hsWglFWmH","outputId":"323f6665-8b91-41eb-ea82-d841ae4c967c","trusted":true,"execution":{"iopub.status.busy":"2025-01-22T04:19:10.890404Z","iopub.execute_input":"2025-01-22T04:19:10.890638Z","iopub.status.idle":"2025-01-22T04:19:11.101788Z","shell.execute_reply.started":"2025-01-22T04:19:10.890617Z","shell.execute_reply":"2025-01-22T04:19:11.101132Z"}},"outputs":[{"name":"stdout","text":"Submission saved to submission.csv\n","output_type":"stream"},{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"                       id                                               text  \\\n0  eng_test_track_a_00001  / o \\ So today I went in for a new exam with D...   \n1  eng_test_track_a_00002  The image I have in my mind is this: a group o...   \n2  eng_test_track_a_00003  I slammed my fist against the door and yelled,...   \n3  eng_test_track_a_00004                       I could not unbend my knees.   \n4  eng_test_track_a_00005  I spent the night at the hotel, mostly hanging...   \n\n   anger  fear  joy  sadness  surprise  \n0      0     0    0        0         0  \n1      0     1    0        1         0  \n2      0     0    1        0         0  \n3      0     0    0        0         0  \n4      0     0    1        0         0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>text</th>\n      <th>anger</th>\n      <th>fear</th>\n      <th>joy</th>\n      <th>sadness</th>\n      <th>surprise</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>eng_test_track_a_00001</td>\n      <td>/ o \\ So today I went in for a new exam with D...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>eng_test_track_a_00002</td>\n      <td>The image I have in my mind is this: a group o...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>eng_test_track_a_00003</td>\n      <td>I slammed my fist against the door and yelled,...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>eng_test_track_a_00004</td>\n      <td>I could not unbend my knees.</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>eng_test_track_a_00005</td>\n      <td>I spent the night at the hotel, mostly hanging...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":26},{"cell_type":"code","source":"","metadata":{"id":"eBUZyMCqULX-","trusted":true},"outputs":[],"execution_count":null}]}